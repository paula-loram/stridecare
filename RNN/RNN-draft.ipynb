{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4793220e-c870-44bb-b0c6-ab718277123b",
   "metadata": {},
   "source": [
    "## RNN Model\n",
    "- What is the input shape going to be?\n",
    "\n",
    "- Target categories we're going to have (6):\n",
    "    - Healthy (NoInjury)\n",
    "    - Knee\n",
    "    - Thigh\n",
    "    - Lower Leg (shin/calves)\n",
    "    - Ankle + Foot\n",
    "    - Hip/Pelvis + LumbarSpine + SI.Joint\n",
    "\n",
    "- We're using LSTM instead of GRU (? .. check)\n",
    "\n",
    "- How many hidden layers / dense layers?\n",
    "\n",
    "- Please Check: activation, loss, optimizer?\n",
    "- Fine-tune hyperparameters: epochs, learning_rate\n",
    "\n",
    "### how can we ensure that the bq file corresponds to the right metadata row name? https://g.co/gemini/share/4af73bcc7f70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63dda415-9172-44a0-aff1-b1e0412ff647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0. Imports\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential, Input, layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9926e817-aad1-455f-b841-cf4b30b74f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee2cc5bd-3175-4288-9711-aa30d1b96f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata_df = pd.read_csv(\"stridecare/data/meta/metadata_for_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e2f4ba6-924f-4958-8942-ebe7543f2b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata_df['bq_name'] = metadata_df['filename'].apply(lambda x: 'angles_' + x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f375d5e0-d96d-40d0-9fc0-563d44753321",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on angles_20101005T132240\n",
      "(10, 12000)\n",
      "working on angles_20101117T132240\n",
      "(10, 12000)\n",
      "working on angles_20120703T102550\n",
      "(10, 12000)\n",
      "working on angles_20120717T103748\n",
      "(10, 12000)\n",
      "working on angles_20120717T105021\n",
      "(10, 12000)\n",
      "working on angles_20120809T100115\n",
      "(10, 12000)\n",
      "working on angles_20120829T125604\n",
      "(10, 12000)\n",
      "working on angles_20121101T095248\n",
      "(10, 12000)\n",
      "working on angles_20121122T140316\n",
      "(10, 12000)\n",
      "working on angles_20130410T105446\n",
      "(10, 12000)\n",
      "working on angles_20130606T134651\n",
      "(10, 12000)\n",
      "working on angles_20130620T121501\n",
      "(10, 12000)\n",
      "working on angles_20130806T105329\n",
      "(10, 12000)\n",
      "working on angles_20130904T115007\n",
      "(10, 12000)\n",
      "working on angles_20130910T105157\n",
      "(10, 12000)\n",
      "working on angles_20130917T114750\n",
      "(10, 12000)\n",
      "working on angles_20130924T105459\n",
      "(10, 12000)\n",
      "working on angles_20130924T115413\n",
      "(10, 12000)\n",
      "working on angles_20131105T103758\n",
      "(10, 12000)\n",
      "working on angles_20131126T104419\n",
      "(10, 12000)\n",
      "working on angles_20131127T115511\n",
      "(10, 12000)\n",
      "working on angles_20140116T125000\n",
      "(10, 12000)\n",
      "working on angles_20140205T110432\n",
      "(10, 12000)\n",
      "working on angles_20110713T114725\n",
      "(10, 12000)\n",
      "working on angles_20110929T094056\n",
      "(10, 12000)\n",
      "working on angles_20111116T124303\n",
      "(10, 12000)\n",
      "working on angles_20120118T113903\n",
      "(10, 12000)\n",
      "working on angles_20120327T114407\n",
      "(10, 12000)\n",
      "working on angles_20120508T105413\n",
      "(10, 12000)\n",
      "working on angles_20110126T125910\n",
      "(10, 12000)\n",
      "working on angles_20110609T103144\n",
      "(10, 12000)\n",
      "working on angles_20110628T105139\n",
      "(10, 12000)\n",
      "working on angles_20140410T152849\n",
      "(10, 12000)\n",
      "working on angles_20140410T153617\n",
      "(10, 12000)\n",
      "working on angles_20140320T120724\n",
      "(10, 12000)\n",
      "working on angles_20140617T103150\n",
      "(10, 12000)\n",
      "working on angles_20140325T151745\n",
      "(10, 12000)\n",
      "working on angles_20140321T102945\n",
      "(10, 12000)\n",
      "working on angles_20140506T165539\n",
      "(10, 12000)\n",
      "working on angles_20140325T090726\n",
      "(10, 12000)\n",
      "working on angles_20140320T165100\n",
      "(10, 12000)\n",
      "working on angles_20140321T164436\n",
      "(10, 12000)\n",
      "working on angles_20140319T174655\n",
      "(10, 12000)\n",
      "working on angles_20160331T100329\n",
      "(10, 12000)\n",
      "working on angles_20140324T102916\n",
      "(10, 12000)\n",
      "working on angles_20160205T083943\n",
      "(10, 12000)\n",
      "working on angles_20160224T082034\n",
      "(10, 12000)\n",
      "working on angles_20140414T094847\n",
      "(10, 12000)\n",
      "working on angles_20140331T103844\n",
      "(10, 12000)\n",
      "working on angles_20140402T132349\n",
      "(10, 12000)\n",
      "working on angles_20140523T113613\n",
      "(10, 12000)\n",
      "working on angles_20140411T092355\n",
      "(10, 12000)\n",
      "working on angles_20140410T160801\n",
      "(10, 12000)\n",
      "working on angles_20160613T124827\n",
      "(10, 12000)\n",
      "working on angles_20151008T130905\n",
      "(10, 12000)\n",
      "working on angles_20140513T164936\n",
      "(10, 12000)\n",
      "working on angles_20140408T132129\n",
      "(10, 12000)\n",
      "working on angles_20140331T141326\n",
      "(10, 12000)\n",
      "working on angles_20140407T102830\n",
      "(10, 12000)\n",
      "working on angles_20140424T122129\n",
      "(10, 12000)\n",
      "working on angles_20140505T170702\n",
      "(10, 12000)\n",
      "working on angles_20140410T132547\n",
      "(10, 12000)\n",
      "working on angles_20140407T121948\n",
      "(10, 12000)\n",
      "working on angles_20140408T134950\n",
      "(10, 12000)\n",
      "working on angles_20160804T113749\n",
      "(10, 12000)\n",
      "working on angles_20140417T172925\n",
      "(10, 12000)\n",
      "working on angles_20140502T125436\n",
      "(10, 12000)\n",
      "working on angles_20140411T100434\n",
      "(10, 12000)\n",
      "working on angles_20140501T122450\n",
      "(10, 12000)\n",
      "working on angles_20150327T091444\n",
      "(10, 12000)\n",
      "working on angles_20140414T103836\n",
      "(10, 12000)\n",
      "working on angles_20150414T142027\n",
      "(10, 12000)\n",
      "working on angles_20140428T095137\n",
      "(10, 12000)\n",
      "working on angles_20140430T102654\n",
      "(10, 12000)\n",
      "working on angles_20140415T132354\n",
      "(10, 12000)\n",
      "working on angles_20140422T143448\n",
      "(10, 12000)\n",
      "working on angles_20150528T112418\n",
      "(10, 12000)\n",
      "working on angles_20150528T132552\n",
      "(10, 12000)\n",
      "working on angles_20140429T142750\n",
      "(10, 12000)\n",
      "working on angles_20150519T153202\n",
      "(10, 12000)\n",
      "working on angles_20150520T100309\n",
      "(10, 12000)\n",
      "working on angles_20140424T165440\n",
      "(10, 12000)\n",
      "working on angles_20150625T111628\n",
      "(10, 12000)\n",
      "working on angles_20150526T162626\n",
      "(10, 12000)\n",
      "working on angles_20150615T111100\n",
      "(10, 12000)\n",
      "working on angles_20150610T075705\n",
      "(10, 12000)\n",
      "working on angles_20150616T091801\n",
      "(10, 12000)\n",
      "working on angles_20150610T092125\n",
      "(10, 12000)\n",
      "working on angles_20150529T101931\n",
      "(10, 12000)\n",
      "working on angles_20150626T070808\n",
      "(10, 12000)\n",
      "working on angles_20140508T151620\n",
      "(10, 12000)\n",
      "working on angles_20150706T084232\n",
      "(10, 12000)\n",
      "working on angles_20150526T114627\n",
      "(10, 12000)\n",
      "working on angles_20150601T103754\n",
      "(10, 12000)\n",
      "working on angles_20150624T060429\n",
      "(10, 12000)\n",
      "working on angles_20150601T080909\n",
      "(10, 12000)\n",
      "working on angles_20150612T071711\n",
      "(10, 12000)\n",
      "working on angles_20140423T150216\n",
      "(10, 12000)\n",
      "working on angles_20150608T102006\n",
      "(10, 12000)\n",
      "working on angles_20150608T123745\n",
      "(10, 12000)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "angles_list = []\n",
    "\n",
    "for i, bq_name in enumerate(metadata_df['bq_name']):\n",
    "    if i >=100:\n",
    "        break\n",
    "\n",
    "    PROJECT = \"stridecare-461809\"\n",
    "    DATASET = \"angle_csvs\"\n",
    "    TABLE = bq_name\n",
    "\n",
    "    query = f\"\"\"       \n",
    "    SELECT *\n",
    "    FROM {PROJECT}.{DATASET}.{TABLE}\n",
    "    \"\"\"\n",
    "    print(f'working on {bq_name}')\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(query)\n",
    "    result = query_job.result()\n",
    "    df = result.to_dataframe()\n",
    "    df = df.drop(columns=['Frame', 'L_ankle_X', 'L_ankle_Y', 'R_ankle_X', 'R_ankle_Y']).T\n",
    "    df_array = np.array(df)\n",
    "    df_array = pad_sequences(df_array, maxlen = 12000, padding=\"post\", truncating='post')\n",
    "    print(df_array.shape)\n",
    "    angles_list.append(df_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "59d0c2d0-e1c1-45f6-8b85-f6847dd3aa91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_angles = np.array(angles_list)\n",
    "\n",
    "# Now, X_angles is a single NumPy array with shape (100, 10, 12000)\n",
    "# (assuming 100 iterations, 10 features, 12000 maxlen)\n",
    "\n",
    "X_angles_train = X_angles[:70]\n",
    "X_angles_test = X_angles[70:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dbdf977a-846b-409d-8dcd-0c82b6649790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_meta = metadata_df[['age', 'Height', 'Weight', 'Gender']]\n",
    "#X_meta_train = X_meta.loc[:69, :]\n",
    "#X_meta_test = X_meta.loc[69:, :]\n",
    "\n",
    "# We only processed the first 100 BQ names for angles_list,\n",
    "# so we should only use the first 100 corresponding metadata rows.\n",
    "X_meta_subset = X_meta.iloc[:100, :] # Get the first 100 rows of metadata\n",
    "\n",
    "X_meta_train = X_meta_subset.iloc[:70, :] # Samples 0-69 from the subset\n",
    "X_meta_test = X_meta_subset.iloc[70:, :] # Samples 70-99 from the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6d8f7a12-f3dd-4e70-a150-757a78c1d325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_meta_train_preproc = scaler.fit_transform(X_meta_train.drop(columns='Gender'))\n",
    "X_meta_test_prerpoc = scaler.transform(X_meta_test.drop(columns='Gender'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dfccee62-d769-443b-9f5a-87e72bca98db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False, drop=\"if_binary\")\n",
    "X_meta_train_gender = ohe.fit_transform(X_meta_train[['Gender']])\n",
    "X_meta_test_gender = ohe.transform(X_meta_test[['Gender']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c25ffcec-e7d2-4f8f-ab47-1852a766eb44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_meta_train_preproc = np.concat((X_meta_train_preproc, X_meta_train_gender), axis = 1)\n",
    "X_meta_test_preproc = np.concat((X_meta_test_prerpoc, X_meta_test_gender), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "203e4631-d8f6-4247-b450-071694972486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = LabelEncoder().fit_transform(metadata_df['InjJoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c75f9c20-82b1-4cce-9035-0dc9056185c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 6)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y[:100])\n",
    "y_train = y[:70]\n",
    "y_test = y[70:]\n",
    "y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "12a1d82d-8bc3-4920-b21a-c35884d1c0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_angles_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9bf52ddc-7171-4429-bb98-1d573132cff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_angles_train: <class 'numpy.ndarray'>\n",
      "Shape of X_angles_train: (70, 10, 12000)\n",
      "Type of X_meta_train_preproc: <class 'numpy.ndarray'>\n",
      "Shape of X_meta_train_preproc: (70, 4)\n",
      "Type of y_train: <class 'numpy.ndarray'>\n",
      "Shape of y_train: (70, 6)\n",
      "Epoch 1/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 870ms/step - accuracy: 0.2009 - loss: 1.7958 - val_accuracy: 0.1333 - val_loss: 1.8012\n",
      "Epoch 2/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 462ms/step - accuracy: 0.2354 - loss: 1.7565 - val_accuracy: 0.1333 - val_loss: 1.8055\n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 454ms/step - accuracy: 0.2366 - loss: 1.7390 - val_accuracy: 0.2667 - val_loss: 1.8110\n",
      "Epoch 4/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 460ms/step - accuracy: 0.3472 - loss: 1.6905 - val_accuracy: 0.2333 - val_loss: 1.8527\n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 444ms/step - accuracy: 0.3440 - loss: 1.6451 - val_accuracy: 0.2667 - val_loss: 1.8990\n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438ms/step - accuracy: 0.3316 - loss: 1.6246 - val_accuracy: 0.2667 - val_loss: 1.9010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f9a80ab5090>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Imports ####\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential, Input, layers\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#### 1. Architecture ####\n",
    "\n",
    "## Creating the sequential & meta inputs ##\n",
    "\n",
    "    # (1.) Time-series input (coordinates)\n",
    "time_input = Input(shape=(10,12000), name='time_series_input') #Defining input layer\n",
    "\n",
    "x = layers.LSTM(256, activation='tanh', return_sequences=True)(time_input) #Stack the layers using the syntax: new_layer()(previous_layer)\n",
    "x = layers.LSTM(128, activation='tanh', return_sequences=True)(x)\n",
    "x = layers.LSTM(64, activation='tanh', return_sequences=False)(x)  # final temporal summary\n",
    "\n",
    "x = layers.Dense(64, activation='relu')(x)  # flatten & process time branch\n",
    "\n",
    "    # (2.) Metadata Input #\n",
    "# Assume: shape=(n_meta_features,)\n",
    "meta_input = Input(shape=(4,), name='meta_input') #do we need to add number of frames, or number of runners ?\n",
    "\n",
    "y = layers.Dense(32, activation='relu')(meta_input)\n",
    "y = layers.Dense(16, activation='relu')(y)\n",
    "\n",
    "\n",
    "## Concatenating the two-inputs ##\n",
    "\n",
    "    # (3.) Concatenate Both Branches #\n",
    "combined = layers.Concatenate()([x, y])\n",
    "z = layers.Dense(64, activation='relu')(combined)\n",
    "z = layers.Dense(6, activation='softmax')(z)  # final classification layer (6 categories)\n",
    "\n",
    "\n",
    "## Instantiating the model ##\n",
    "\n",
    "model = Model(inputs=[time_input, meta_input], outputs=z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Compiling the model ####\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Fit ####\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=5,\n",
    "                           # min_delta=0.001,(??)\n",
    "                           restore_best_weights=True)\n",
    "##checking sizes\n",
    "print(f\"Type of X_angles_train: {type(X_angles_train)}\")\n",
    "print(f\"Shape of X_angles_train: {X_angles_train.shape}\")\n",
    "print(f\"Type of X_meta_train_preproc: {type(X_meta_train_preproc)}\")\n",
    "print(f\"Shape of X_meta_train_preproc: {X_meta_train_preproc.shape}\")\n",
    "print(f\"Type of y_train: {type(y_train)}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "\n",
    "model.fit([X_angles_train, X_meta_train_preproc], y_train, \n",
    "          epochs=50, \n",
    "          batch_size=32, \n",
    "          validation_data = ([X_angles_test, X_meta_test_preproc], y_test),\n",
    "          verbose=1,\n",
    "          callbacks=[early_stop])\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Predict ####\n",
    "#model.predict(??, ??)\n",
    "          \n",
    "\n",
    "# 5. Evaluation\n",
    "#model.evaluate(??, ??)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0aa4ff28-e9d1-4f9e-b8cc-f9092b6eb3ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_angles_train)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
