{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b1aac1a-e4d9-48cc-b7f7-ace612b67b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from training_data_angles import analyze_gait_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283cc90-3269-4767-9bdb-be998adaf25a",
   "metadata": {},
   "source": [
    "## Notebook aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542d5be-8e0a-48de-8324-15d15d9d606e",
   "metadata": {},
   "source": [
    "The aim of this notebook is to load from bigquery the .json files containing the coordinates for runners, then obtain the angles using the function in training_data_angles.py, to output each one into a new folder in bigquery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08310d-0476-4ecf-a62a-2058a9d6c963",
   "metadata": {},
   "source": [
    "### Creating list of filenames to loop through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39afdbc8-8b7a-48fa-97d0-cad6787fc106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jsons = pd.read_csv(\"data/meta/run_data_meta.csv\")\n",
    "\n",
    "json_filenames = []\n",
    "for i in range(len(jsons)):\n",
    "    json_filenames.append(f\"{jsons.loc[i, 'sub_id']}/{jsons.loc[i,'filename']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd022a5-c233-4b69-bf0f-a53366290eb6",
   "metadata": {},
   "source": [
    "### Google Cloud Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "017f0e30-834e-47b6-a77b-b762bf595d48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gait analysis pipeline for GCS input bucket: stridecare/RunInjuryClinic/\n",
      "Processed angles will be uploaded to BigQuery dataset: stridecare-461809.ange_csvs\n",
      "Processed CSVs will also be saved locally to: ./processed_angles_data\n"
     ]
    }
   ],
   "source": [
    "# Input bucket name\n",
    "GCS_INPUT_BUCKET_NAME = 'stridecare'\n",
    "GCS_INPUT_PREFIX = 'RunInjuryClinic/' \n",
    "\n",
    "# Initialize GCS client\n",
    "gcs_client = storage.Client()\n",
    "\n",
    "#Output to bigquery\n",
    "BQ_PROJECT_ID = 'stridecare-461809'\n",
    "BQ_DATASET_ID = 'angle_csvs'\n",
    "\n",
    "#output to local data folder\n",
    "LOCAL_OUTPUT_BASE_DIR = './processed_angles_data'\n",
    "\n",
    "# Temporary directory to download JSON files to\n",
    "LOCAL_TEMP_JSON_DIR = f\"temp_gait_json_files_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "os.makedirs(LOCAL_TEMP_JSON_DIR, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "# Initialize GCS and BigQuery clients\n",
    "gcs_client = storage.Client()\n",
    "bq_client = bigquery.Client(project=BQ_PROJECT_ID)\n",
    "\n",
    "print(f\"Starting gait analysis pipeline for GCS input bucket: {GCS_INPUT_BUCKET_NAME}/{GCS_INPUT_PREFIX}\")\n",
    "print(f\"Processed angles will be uploaded to BigQuery dataset: {BQ_PROJECT_ID}.{BQ_DATASET_ID}\")\n",
    "print(f\"Processed CSVs will also be saved locally to: {LOCAL_OUTPUT_BASE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f44b5-53cb-4f93-92b0-2b6708ed7841",
   "metadata": {},
   "source": [
    "### Safety check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "544401b9-3a76-4e2f-a2ac-84016759519b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1715677448.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"BigQuery Dataset '{BQ_DATASET_ID}' created.\")\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    input_bucket = gcs_client.bucket(GCS_INPUT_BUCKET_NAME)\n",
    "\n",
    "    # Ensure the BigQuery dataset exists\n",
    "    try:\n",
    "        bq_client.get_dataset(BQ_DATASET_ID)\n",
    "        print(f\"BigQuery Dataset '{BQ_DATASET_ID}' already exists.\")\n",
    "    except Exception: # Catching a general exception if dataset not found, BigQuery raises NotFound\n",
    "        print(f\"BigQuery Dataset '{BQ_DATASET_ID}' not found. Creating it...\")\n",
    "        dataset = bigquery.Dataset(f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}\")\n",
    "        dataset.location = \"EU\" # Or your desired location (e.g., \"US\", \"asia-east1\")\n",
    "        bq_client.create_dataset(dataset, timeout=30)\n",
    "        print(f\"BigQuery Dataset '{BQ_DATASET_ID}' created successfully.\")\n",
    "\n",
    "\n",
    "    # --- THIS SECTION IS UPDATED TO USE YOUR LIST FORMAT ---\n",
    "    # Your list directly from json_list.ipynb\n",
    "    full_paths_from_list_ipynb = [\n",
    "        '100433/20101005T132240.json',\n",
    "        '100434/20101117T132240.json',\n",
    "        '100537/20120703T102550.json',\n",
    "        '100560/20120717T103748.json',\n",
    "        '101481/20120717T105021.json',\n",
    "        '100591/20120809T100115.json',\n",
    "        '100595/20120829T125604.json',\n",
    "        '100646/20121101T095248.json',\n",
    "        # Add all other entries from your json_list.ipynb\n",
    "    ]\n",
    "\n",
    "    json_files_to_process = []\n",
    "    for full_path in full_paths_from_list_ipynb:\n",
    "        parts = full_path.split('/')\n",
    "        if len(parts) >= 2:\n",
    "            json_files_to_process.append({\"sub_id\": parts[-2], \"json_file\": parts[-1]})\n",
    "        else:\n",
    "            print(f\"Warning: Skipping invalid path format in list: {full_path}\")\n",
    "            \n",
    "    # --- END OF UPDATED SECTION ---\n",
    "\n",
    "\n",
    "    if not json_files_to_process:\n",
    "        print(\"No JSON files specified in 'json_files_to_process'. Please ensure your list is populated.\")\n",
    "    \n",
    "    processed_files_count = 0\n",
    "    \n",
    "    for entry in json_files_to_process:\n",
    "        sub_id = entry.get(\"sub_id\")\n",
    "        json_filename = entry.get(\"json_file\")\n",
    "\n",
    "        if not sub_id or not json_filename:\n",
    "            print(f\"Skipping invalid entry in list: {entry}\")\n",
    "            continue\n",
    "\n",
    "        # Construct the full storage path for the input JSON file in GCS\n",
    "        gcs_input_blob_path = f\"{GCS_INPUT_PREFIX}{sub_id}/{json_filename}\"\n",
    "        \n",
    "        print(f\"\\nProcessing GCS blob: {gcs_input_blob_path}\")\n",
    "        local_json_filepath = os.path.join(LOCAL_TEMP_JSON_DIR, json_filename)\n",
    "        \n",
    "        try:\n",
    "            # Download the JSON file locally\n",
    "            input_blob = input_bucket.blob(gcs_input_blob_path)\n",
    "            input_blob.download_to_filename(local_json_filepath)\n",
    "            print(f\"Downloaded '{gcs_input_blob_path}' to '{local_json_filepath}'\")\n",
    "\n",
    "            # Process the file using the imported analyze_gait_data function\n",
    "            angles_df = analyze_gait_data(local_json_filepath)\n",
    "\n",
    "            if not angles_df.empty:\n",
    "                base_json_name = os.path.splitext(json_filename)[0] # e.g., \"20110531T161051\"\n",
    "                \n",
    "                # --- 1. Save to local data folder (as CSV) ---\n",
    "                local_output_sub_dir = os.path.join(LOCAL_OUTPUT_BASE_DIR, sub_id)\n",
    "                os.makedirs(local_output_sub_dir, exist_ok=True) # Create sub_id folder locally\n",
    "                local_csv_filename = f\"{base_json_name}_angles.csv\"\n",
    "                local_csv_filepath = os.path.join(local_output_sub_dir, local_csv_filename)\n",
    "                \n",
    "                angles_df.to_csv(local_csv_filepath, index=False)\n",
    "                print(f\"Saved angles locally to: '{local_csv_filepath}'\")\n",
    "\n",
    "                # --- 2. Upload to BigQuery (as a new table) ---\n",
    "                # BigQuery table names must start with a letter or underscore and contain\n",
    "                # only letters, numbers, and underscores. Replace any invalid chars.\n",
    "                bq_table_name = f\"angles_{base_json_name.replace('.', '_').replace('-', '_')}\"\n",
    "                bq_table_id = f\"{BQ_PROJECT_ID}.{BQ_DATASET_ID}.{bq_table_name}\"\n",
    "                \n",
    "                # Configure BigQuery load job (e.g., overwrite existing table if it has the same name)\n",
    "                job_config = bigquery.LoadJobConfig(\n",
    "                    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE, # Overwrites if table exists\n",
    "                    # Consider WRITE_APPEND if you want to add rows to an existing table with the same schema\n",
    "                    # or WRITE_EMPTY if you want the job to fail if the table already exists.\n",
    "                )\n",
    "\n",
    "                load_job = bq_client.load_table_from_dataframe(\n",
    "                    angles_df, bq_table_id, job_config=job_config\n",
    "                )\n",
    "                load_job.result() # Wait for the job to complete\n",
    "                \n",
    "                print(f\"Uploaded angles to BigQuery table: '{bq_table_id}'\")\n",
    "                processed_files_count += 1\n",
    "            else:\n",
    "                print(f\"  '{json_filename}': No angle data returned by analyze_gait_data. Skipping all outputs.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing '{gcs_input_blob_path}': {e}\")\n",
    "        finally:\n",
    "            # Clean up: remove the downloaded JSON file\n",
    "            if os.path.exists(local_json_filepath):\n",
    "                os.remove(local_json_filepath)\n",
    "                print(f\"Cleaned up local temporary JSON file: '{local_json_filepath}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unhandled error occurred: {e}\")\n",
    "finally:\n",
    "    # Clean up the temporary local JSON download directory\n",
    "    if os.path.exists(LOCAL_TEMP_JSON_DIR):\n",
    "        try:\n",
    "            os.rmdir(LOCAL_TEMP_JSON_DIR) # Will only remove if empty\n",
    "            print(f\"Cleaned up temporary JSON download directory: '{LOCAL_TEMP_JSON_DIR}'\")\n",
    "        except OSError:\n",
    "            print(f\"Warning: Could not remove non-empty temporary JSON directory: '{LOCAL_TEMP_JSON_DIR}'. Please remove manually if empty.\")\n",
    "\n",
    "print(f\"\\nProcessing complete. Total files processed and outputted: {processed_files_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8f8b3-4633-4dd6-abe2-2ce08679b8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
